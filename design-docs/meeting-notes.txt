2013-01-31
----------


Threading

deamon
deamon /port or 1 for the system?

1 deamon / node 

but still work on both subnets

address file which indicates which hcas/ports to run on.

daemon would be connected to multiple distribution trees based on port.  Even
if multiple ports on the same subnet.

   join/port and multiple joins for each pkey.
   May NOT need the pkey in MemberRecord since the master knows the pkey's
   available on the port.

Main control thread sending/recv umad

Use socket pairs within process to communicate among threads.

for example a reparent would need to tell the parent communication thread to
   cease - disconnect - and reconnect and redo.

	However, We can defer state changes to where convenient.

We should make all communications client initiated (always request/response)
with the exception of the epoch which is a push (small push).

Parents notice a connection is dropped they would notify the master that they
lost their children.

digress into talking about failure modes...



Overall the threads would be:
	1 control (handles listen for connects and umad processing)
	1 up parent (or more?; get data from parent)
	N group down to children
		(thread pool processing requests from the children)
		do the requests have to be serialized?

		Group the children into a single thread?
		   That makes internal data structures easier.
		   However, if requests are not balanced you may end up with
		   less threads doing all the work.


Database keys vs record ids:

	Any record is id'ed based on the offset within the table.
	all records have an index (offset)

	meta data would define the guid to offset mapping


Tables with variable sized records would have 2 tables

	Fixed sized records have offsets into the variable sized table.


Keys may not always be a guid.

	each table has it's own key size.





Results of meeting 2013-01-16
-----------------------------

Sean 2 methods of locking.

Read the data out checking log and applying changes.

Locks for large updates.


Ira to continue working on the plugin and getting a simple test client
"ibssaquery" connected

Hal to work on his plugin and the copy of the data out of OpenSM with large
simulated networks.

Sean to continue with DB design/updates to rdma_cm for AF_IB




Results of meeting 2012-12-20
-----------------------------

Need to compare epoch's as clients may be ahead of parents.

Need to add connecting state back into diagram emailed out.

Secondary path is promoted to primary when primary is lost or not available.

If master then "AppSet's" the current connection as primary the client stays
connected.  ie this AppSet is a no-op.

Look at APM state machine to allow the migration of Alternate paths within the
"primary/secondary" parents connected state.  I.e. the alternate path is not
a separate state for the client.

Parents will always be ready for clients before joining the tree.  That way
they should never fail a connection without actual errors.

Should probably support a message for the master to request current parent
information.



Results of Meeting 2012-10-10
-----------------------------


SM plugin server to use umad vendor specific class for hook up.

Ira to start header file to get this going and start getting clients hooked
up.



int tot_init(int num_retry, int timeout_ms)

int tot_reg_cb...
Not doing the call back.

int tot_hello(char *tree, uint32_t QPN, uint16_t level)

int tot_hookup(uint16_t lid, uint32_t QPN)




Results of Meeting 2012-09-27
-----------------------------

4 types of nodes.

	1) Root node
	2) "Comm" node
	3) SSA node
	4) clients

What do we do with the root data?  Full data or transactional?





Results of Meeting 2012-09-13
-----------------------------

Only some nodes know the topology.  That is fine.

Hello
=====

How do they know the level in the tree 2 options.
	1) config file knowing the level to send in "hello"
	2) ???


Do the leaf nodes join the "tree"?  Not sure.

Hello _does_ _not_ get an immediate response.

This is probably better with an "ack" message but the "ack" does not have the
"parent" info.  Ack could have timeout for the client to expect it's parent
message.


Parent
======

"Data" response.


Hookup
======




So use VS MAD to do the above messages.  QP1 will not see much traffic.







Results of Meeting 2012-09-06
-----------------------------

How many copies of the DB do we want?

Wait for the ACM to reissue query...

Request all data from SSA.

ibacm is going to have dual DB and swap over.


What is the granularity of the Epoch???

Sean was picturing a global 1 value and then an ibacm node would do a full
update to the latest version.

Eitan was worried about the granularity of the queries.  All to All can not
afford doing 1 PR at a time.

The query will be "now I have permission" to do the update.

Maybe just need 2 on each node?  Just a "new" and the "working".


*************

ibacm supplies just the old epoch for update request.

The SSA then provides ALL data for that epoch to current epoch.

*************


What about using a vendor MAD?
------------------------------

NO just do a single SR query and then connect to the "master" SSA and it will
redirect you.



What about packaging...
-----------------------

Just sharing header files.

put headers in libibumad

Sean was just thinking some header files for the message definitions


This is non-standard...
Have to pad out for message size.  Unless you stream messages...

Versions or padding out and grow rarely.  Or do something with streaming where
the header contains message length???

   ?? rsockets... ??

Base all messages off of existing MAD header...  with some modifications like
no RMPP and add message length.

Control messages have header but RDMA messages are "raw" data.

Control Header format...  What should that be?

	Seq Num
	message type
	message length (? length could be determined by type)
	epoch (? Or is this message data?)
	data...

Define SSA connection end point info
	type (father/mother/grandfather)
	connection channel (UD QPn)
	ConnCh PathRecord (list?)





Meeting 2012-08-30
------------------


"Dark" wire up for SSA service

Assumptions:

	1) although this protocol is designed to wire up a cluster from cold
	   boot it is probably more efficient for the SSA nodes to be booted
	   prior to the compute nodes (ibacm nodes) booting.  The SM and SSA's
	   can then have a logical tree amongst themselves prior to the ibacm
	   nodes coming online.

	2) nodes can come and go.  All wire up is done in a dynamic fashion for
	   both ibacm and SSA nodes.




SSA's can join mcast group for heartbeat communications.  This should scale as
there were be relatively few of them.  When they come online they send messages
to the SM/Master SSA that they exist.

The Master SSA will determine where in the physical and logical topology the
SSA's should be located.



Multicast vs SR for initial wire up.


Use SR to find initial join to tree.

Once joined we get 2 parents.  (Primary and backup)

After this the client does not need to do a "dark query" for the tree, unless
the primary _and_ backup fail.








Push info up and down tree.  Similar data connection???

Data for up tree (messages known for now)

	1) inform info
	2) names for resolution (IP's, hostnames)


Down is: (RDMA and messages)

	1) SA DB
	2) events
	3) Name resolution (invent new things here)


So who controls the data?

When data changes; Do we push data down or pull it down?

This could be different depending on where in the tree we are.

Data is collected by a single node and timestamps that with a single epoch...

How do we know that we have collected enough data to publish to all nodes?

What about an algorithm which controls by time or number of nodes?

Path records depend on the source not just the destination...


3 "pieces"

plugin
SSA (daemon)
	Plugin
ibacm

Within this a library package

So 4 packages (current ibacm, SSA, plugin, and common libraries)

I am on the hook to do the distribution layer.


