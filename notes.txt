Results of Meeting 2012-09-06
-----------------------------

How many copies of the DB do we want?

Wait for the ACM to reissue query...

Request all data from SSA.

ibacm is going to have dual DB and swap over.


What is the granularity of the Epoch???

Sean was picturing a global 1 value and then an ibacm node would do a full
update to the latest version.

Eitan was worried about the granularity of the queries.  All to All can not
afford doing 1 PR at a time.

The query will be "now I have permission" to do the update.

Maybe just need 2 on each node?  Just a "new" and the "working".


*************

ibacm supplies just the old epoch for update request.

The SSA then provides ALL data for that epoch to current epoch.

*************


What about using a vendor MAD?
------------------------------

NO just do a single SR query and then connect to the "master" SSA and it will
redirect you.



What about packaging...
-----------------------

Just sharing header files.

put headers in libibumad

Sean was just thinking some header files for the message definitions


This is non-standard...
Have to pad out for message size.  Unless you stream messages...

Versions or padding out and grow rarely.  Or do something with streaming where
the header contains message length???

   ?? rsockets... ??

Base all messages off of existing MAD header...  with some modifications like
no RMPP and add message length.

Control messages have header but RDMA messages are "raw" data.

Control Header format...  What should that be?

	Seq Num
	message type
	message length (? length could be determined by type)
	epoch (? Or is this message data?)
	data...

Define SSA connection end point info
	type (father/mother/grandfather)
	connection channel (UD QPn)
	ConnCh PathRecord (list?)





Meeting 2012-08-30
------------------


"Dark" wire up for SSA service

Assumptions:

	1) although this protocol is designed to wire up a cluster from cold
	   boot it is probably more efficient for the SSA nodes to be booted
	   prior to the compute nodes (ibacm nodes) booting.  The SM and SSA's
	   can then have a logical tree amongst themselves prior to the ibacm
	   nodes coming online.

	2) nodes can come and go.  All wire up is done in a dynamic fashion for
	   both ibacm and SSA nodes.




SSA's can join mcast group for heartbeat communications.  This should scale as
there were be relatively few of them.  When they come online they send messages
to the SM/Master SSA that they exist.

The Master SSA will determine where in the physical and logical topology the
SSA's should be located.



Multicast vs SR for initial wire up.


Use SR to find initial join to tree.

Once joined we get 2 parents.  (Primary and backup)

After this the client does not need to do a "dark query" for the tree, unless
the primary _and_ backup fail.








Push info up and down tree.  Similar data connection???

Data for up tree (messages known for now)

	1) inform info
	2) names for resolution (IP's, hostnames)


Down is: (RDMA and messages)

	1) SA DB
	2) events
	3) Name resolution (invent new things here)


So who controls the data?

When data changes; Do we push data down or pull it down?

This could be different depending on where in the tree we are.

Data is collected by a single node and timestamps that with a single epoch...

How do we know that we have collected enough data to publish to all nodes?

What about an algorithm which controls by time or number of nodes?

Path records depend on the source not just the destination...


3 "pieces"

plugin
SSA (daemon)
	Plugin
ibacm

Within this a library package

So 4 packages (current ibacm, SSA, plugin, and common libraries)

I am on the hook to do the distribution layer.


